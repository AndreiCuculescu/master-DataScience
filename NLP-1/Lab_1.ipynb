{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbGSaGQ9vIS3"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07IDg6-UvIS6"
      },
      "source": [
        "#### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzlCrIivIS7",
        "outputId": "45e28676-6fd4-4129-bd1b-50e7b1896973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58lHrhEvIS9"
      },
      "source": [
        "#### Download models or corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQRlgElvIS9",
        "outputId": "de67abef-5cfb-40d9-8f25-226752638bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 1113, in _interactive_download\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 1394, in __init__\n",
            "    top = self.top = Tk()\n",
            "  File \"/usr/lib/python3.8/tkinter/__init__.py\", line 2270, in __init__\n",
            "    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n",
            "_tkinter.TclError: no display name and no $DISPLAY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 2550, in <module>\n",
            "    downloader.download(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 763, in download\n",
            "    self._interactive_download()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 1115, in _interactive_download\n",
            "    DownloaderShell(self).run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nltk/downloader.py\", line 1141, in run\n",
            "    user_input = input(\"Downloader> \").strip()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# import nltk\n",
        "!python -m nltk.downloader # shows a window when graphical output available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvj0wdJmvIS-",
        "outputId": "def53db6-dde1-40a1-800c-0a0e37c81e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQEsyB5JvITB"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN3HUcBEvIS_"
      },
      "outputs": [],
      "source": [
        "tweet = \"RT @lOR42wsOEFcv3f: I fall too fast, crash too hard, forgive too easily and care too much... :( #amiright\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kl8O_CcvITA"
      },
      "outputs": [],
      "source": [
        "query = 'fast'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFK_-15yvIS9"
      },
      "source": [
        "The naive way..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz3v_jj0vITA",
        "outputId": "ab28fd97-7694-4146-c9bc-28132e3b0a74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tweet.find(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xhFpKFpvITB",
        "outputId": "8d840e94-c596-42d0-cfe2-c4138ee1d9f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@lOR42wsOEFcv3f:',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast,',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard,',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tweet.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sx9xgcFvITC",
        "outputId": "fcf6de6e-ffda-41bc-f1bb-4d6e5a009ee8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "[query in tweet.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct tokenization: informed splitting of the text into tokens"
      ],
      "metadata": {
        "id": "gjhBVUZK1LV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoieLiJcvITC",
        "outputId": "a6629bee-2f86-4b53-f4d3-468e9c529c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA6xDrMdvITC",
        "outputId": "f796a91f-c8d5-4363-bb20-504322646df4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "[query in nltk.word_tokenize(tweet)]\n",
        "# query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5qgGixvITD",
        "outputId": "dc26bb15-84bd-49cb-9f6f-b741a36b80b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet, language='spanish')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More options..."
      ],
      "metadata": {
        "id": "-KBibnjA1b26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDcB89vWvITD"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "custom_tokenizer = RegexpTokenizer('[a-zA-Z0-9]*', discard_empty=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbfG3liKvITE",
        "outputId": "2d2c9bdf-d676-46d6-ed91-72635235e9e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '',\n",
              " '',\n",
              " 'lOR42wsOEFcv3f',\n",
              " '',\n",
              " '',\n",
              " 'I',\n",
              " '',\n",
              " 'fall',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'fast',\n",
              " '',\n",
              " '',\n",
              " 'crash',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'hard',\n",
              " '',\n",
              " '',\n",
              " 'forgive',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'easily',\n",
              " '',\n",
              " 'and',\n",
              " '',\n",
              " 'care',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'much',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'amiright',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "custom_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUojQmtRvITE"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm8kis0bvITE",
        "outputId": "0b0954fc-0d0d-4538-e521-c8e160e481d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yltAQxCvITE",
        "outputId": "f18e8c60-f02f-4e7e-e6b9-48917bfe2438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too_fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe = MWETokenizer()\n",
        "mwe.add_mwe(('too', 'fast'))\n",
        "mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fDVPDRnvITF"
      },
      "outputs": [],
      "source": [
        "mwe.add_mwe((('too', 'fast'), ('too', 'hard')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugIC2npavITF",
        "outputId": "7da9db9c-b6a6-4354-963c-712bf5997f5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "query = 'fast'\n",
        "query in mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-5pXTyvITG"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bRK61fGevITG",
        "outputId": "39683681-4b5a-4dcf-b429-c25aa1d22600"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rt @lor42wsoefcv3f: i fall too fast, crash too hard, forgive too easily and care too much... :( #amiright'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "tweet.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4-6Rnr7vITG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_tokens(tokenized_text):\n",
        "    # Lowercase\n",
        "    tokens = [t.lower() for t in tokenized_text]\n",
        "    # Remove hashtags\n",
        "    tokens = [t for t in tokens if not t.startswith('#')]\n",
        "    # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    # Keep only letters\n",
        "#     tokens = [t for t in tokens if re.match('^[a-z]+$', t)]\n",
        "    # Normalize characters\n",
        "    tokens = [re.sub('á', 'a', t) for t in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edRDrifZvITG",
        "outputId": "1344a66e-785b-466e-b3d6-29db08836775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muy', 'rapido']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "spanish_query = 'muy rápido'\n",
        "normalize_tokens(tweet_tokenizer.tokenize(spanish_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "HFpAR4b_vITH",
        "outputId": "92176d71-949e-4e14-9421-4c1a9e2e6e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'muy rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "import unidecode\n",
        "unidecode.unidecode(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5AUDgevITH",
        "outputId": "b8515831-44aa-4ba4-8100-24913f35ab82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "normalize_tokens(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLG3Pd4SvITH"
      },
      "source": [
        "#### Uniform normalization principle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491ipWWuvITH",
        "outputId": "ada5680c-ef92-4ef2-aa0c-32678c06fab4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['too', 'fast', 'too', 'furious']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "query = 'TOO fast TOO furious'\n",
        "tokenized_query = tweet_tokenizer.tokenize(query)\n",
        "normalized_query = normalize_tokens(tokenized_query)\n",
        "# normalized_query = tokenized_query\n",
        "normalized_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfpl7C7nvITH",
        "outputId": "b3fa93e8-8aaf-49d7-e2f7-b47ddaf9abfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "# normalized_tweet = normalize_tokens(tweet.split())\n",
        "normalized_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "truJmsHpvITI",
        "outputId": "6d7e003c-8286-407a-cd03-e6a12b479630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'too', 'fast'}\n",
            "2 common word(s)\n"
          ]
        }
      ],
      "source": [
        "common_words = set(normalized_query).intersection(normalized_tweet)\n",
        "print(common_words)\n",
        "print(len(common_words), \"common word(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaLxOlOMvITJ"
      },
      "source": [
        "#### Stemming / Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pmCSIy4vITJ"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoXs3q-SvITJ",
        "outputId": "23d4f3ac-f81c-43b3-c7c6-9da351a118fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.LancasterStemmer() # is prone to overstemming\n",
        "[stemmer.stem(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mWZSR5d4OuH",
        "outputId": "7ad17989-7fe4-4f5e-b2d2-e1af6525f48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fal',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forg',\n",
              " 'too',\n",
              " 'easy',\n",
              " 'and',\n",
              " 'car',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxv9vv3TvITK",
        "outputId": "9ac98128-2587-4275-a39d-5eb28dac19ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english') # Porter2\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"running\"))\n",
        "\n",
        "print(stemmer.stem(\"runs\"))\n",
        "\n",
        "print(stemmer.stem(\"ran\"))\n",
        "\n",
        "print(stemmer.stem(\"darling\"))\n",
        "\n",
        "print(stemmer.stem(\"are\"))\n",
        "\n",
        "print(stemmer.stem(\"bring\"))\n",
        "\n",
        "print(stemmer.stem(\"being\"))\n",
        "\n",
        "print(stemmer.stem(\"Charles\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEDgnkOE4pza",
        "outputId": "983d9492-3f55-4bf1-ae21-e5571a65d648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "darl\n",
            "are\n",
            "bring\n",
            "be\n",
            "charl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1caMRHT7vITK",
        "outputId": "0b898476-d863-40e4-eb86-a23e174b2a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yhe4EZvITK",
        "outputId": "bb2e1eff-826e-4617-d09e-d6b7e009d0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rt', 'NN'), ('i', 'NN'), ('fall', 'VBP'), ('too', 'RB'), ('fast', 'JJ'), ('crash', 'NN'), ('too', 'RB'), ('hard', 'JJ'), ('forgive', 'JJ'), ('too', 'RB'), ('easily', 'RB'), ('and', 'CC'), ('care', 'VB'), ('too', 'RB'), ('much', 'JJ'), ('...', ':'), (':(', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "tagged_tweet = nltk.pos_tag(normalized_tweet) # More on this next lab...\n",
        "print(tagged_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yww_9WdPvITK"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "tag_map = {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV, 'N': wn.NOUN}\n",
        "def get_lemmas(tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    return [lemmatizer.lemmatize(w, pos=tag_map.get(p[0], wn.NOUN)) for (w, p) in tagged_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwkQvd-SvITL",
        "outputId": "0c00e2b9-0dd9-48c0-e39a-3a7a99399a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "query = \"the fastest!\"\n",
        "normalized_query = normalize_tokens(tweet_tokenizer.tokenize(query))\n",
        "print(normalized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AjyEA5vITL",
        "outputId": "2de513c2-d67f-4cc1-f2df-6cfb5b44dd9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rt', 'i', 'fall', 'too', 'fast', 'crash', 'too', 'hard', 'forgive', 'too', 'easily', 'and', 'care', 'too', 'much', '...', ':(']\n",
            "['the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "lemmatized_query = get_lemmas(normalized_query)\n",
        "print(lemmatized_tweet)\n",
        "print(lemmatized_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R08IvRkUvITL",
        "outputId": "8dc8e6d6-8211-4035-baaa-67f622c94574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "normalized_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P9ov1TJ67Ml",
        "outputId": "19009052-0c8d-4ad7-9e4c-408e7a2b57bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24QRJbnIvITM",
        "outputId": "c1617013-f687-4128-bf10-8ca70c2e8121"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "get_lemmas(normalized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq3JncFSvITL",
        "outputId": "e4839287-42c0-4436-e2f5-7975206a3fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common words: {'fast'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Common words:\", set(lemmatized_tweet).intersection(set(lemmatized_query)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exJwCbhSvITI"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R7abjAkvITI",
        "outputId": "bd1781a4-a550-444f-c18b-9b9096e8ff8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu8sWb4vvITI",
        "outputId": "26e204c5-ed0f-4dde-ea67-75f204de6c28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNzURg2kvITJ"
      },
      "outputs": [],
      "source": [
        "blacklist_words = stopwords.words('english') + ['rt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-0g081PvITJ",
        "outputId": "90b38058-bbb7-40ff-c8d1-490b9dc29de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fast', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "cleaned_tweet = [t for t in normalized_tweet if t not in blacklist_words]\n",
        "print(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnD0SkZNvITM"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xckCNborvITM",
        "outputId": "2f504f04-a2a3-4d77-e8a0-244f3b42f167"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 2), ('be', 2), ('fast', 2), ('so', 1), ('the', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(get_lemmas(normalized_tweet)).most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOLFMVSFvITM",
        "outputId": "9d111560-2302-4578-b072-eba787430ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "print(lemmatized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR5W_IeqvITM",
        "outputId": "a7b75f19-5317-40ed-bee6-86c7b9a5c9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'i': 2, 'am': 2, 'so': 1, 'fast': 1, 'the': 1, 'fastest': 1})\n",
            "Counter({'i': 2, 'be': 2, 'fast': 2, 'so': 1, 'the': 1})\n"
          ]
        }
      ],
      "source": [
        "print(Counter(normalized_tweet))\n",
        "print(Counter(lemmatized_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VIE2dhCvITM"
      },
      "source": [
        "#### Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDhiS2VGvITN"
      },
      "outputs": [],
      "source": [
        "query = \"I am too fast. I am too furious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPADLll5vITN"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pp_45fMvITN",
        "outputId": "3db00287-92b6-40ac-fcc5-e83126c28b27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am too fast.', 'I am too furious.']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "sent_tokenize(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrVvihFvITN",
        "outputId": "5ef37c31-9248-4916-b8fb-4708a58da4ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy muy rápido!', 'Estoy muy furioso!']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "spanish_query = 'Soy muy rápido! Estoy muy furioso!'\n",
        "spanish_tokenizer.tokenize(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PuN1DjvvITN",
        "outputId": "151ca934-980b-42ff-ebdd-1e8b06cb384e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J.K. Rowling is rich.', 'I am not as rich as J.K.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "sent_tokenize(\"J.K. Rowling is rich. I am not as rich as J.K.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJaD2GydvITO"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "PunktSentenceTokenizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mcm9gIBvITO"
      },
      "source": [
        "#### Numeral conversion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number\n",
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRsqSRbzuDk",
        "outputId": "a6af91e6-0c86-480b-99a3-d9a02feda701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=0af1bb44a5ddd3f08db33c846318b132b0af2700cd9e3d281259bb4041ebd884\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=017624a0d66cb4d217aba7b14e95f96dedbe837ff3e500884c55e9fe2ba18098\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbn8ybBLvITO",
        "outputId": "e9eeceec-6dd6-42c2-ac59-e1aaa9178604"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "import word2number\n",
        "from word2number import w2n\n",
        "w2n.word_to_num(\"eleven\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"twenty three\")"
      ],
      "metadata": {
        "id": "vpA1EUb5z-CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd807685-c09e-4dec-8bad-012ff933adb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "couI08gLvITO"
      },
      "outputs": [],
      "source": [
        "from num2words import num2words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G8h7XGcKJbw0",
        "outputId": "24992eba-a741-4a0b-ee40-37c1d53af209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'twelve'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(101)\n"
      ],
      "metadata": {
        "id": "mjtdQ_n60B2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db137f46-1337-48b7-9251-bdfcb0e78125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one hundred and one'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(2020)"
      ],
      "metadata": {
        "id": "YAK0RUY_0Dnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "165c09d1-51b4-480a-db7c-b0fd236e9c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'two thousand and twenty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"Twelve o'clock!\")"
      ],
      "metadata": {
        "id": "wCvn9L_W7TDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ad0cec-4095-4409-afb9-69ef0ca2e96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise (1p)\n"
      ],
      "metadata": {
        "id": "lM4HWS8I_hUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a recent news article online.\n",
        "Read it in a python variable (input it manually or read from a file).\n",
        "\n",
        "Write a function that normalizes the text and splits it into tokens. Add flags to customize the different preprocessing choices (which stemmer/lemmatizer to use, whether to lowercase, whether to convert numbers, whether to remove stopwords, ...).\n",
        "\n",
        "Store the vocabulary of unique tokens found in the text.\n",
        "\n",
        "Compare the number of unique tokens (\"types\") with different preprocessing settings."
      ],
      "metadata": {
        "id": "7VJBKi1B_lZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs-1SowdnpeU",
        "outputId": "fcc9d654-cac6-4519-b866-6d08981383a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=caf650518d5b07d2f3148065a2851bef4a9ace15653ce3186e67aa4b86ad98f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from num2words import num2words"
      ],
      "metadata": {
        "id": "ZgCmfe0UjO7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY551t2upgD2",
        "outputId": "35192759-75c2-44d2-9ed5-3de72dfb00c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"sea turtle, any of seven species of marine turtles belonging to the families Dermochelyidae (leatherback sea turtles) and Cheloniidae (green turtles, flatback sea turtles, loggerhead sea turtles, hawksbills, and ridleys). Both families are highly aquatic, and most species only appear on coastal beaches for egg laying; however, the green turtle (Chelonia mydas) occasionally basks in terrestrial environments. Adult sea turtles are mainly denizens of tropical and subtropical seas, but the juveniles of both families occur naturally in more temperate waters.Dermochelyids and cheloniids are distantly related; their divergence from one another took place between 100 million and 150 million years ago. Nevertheless, both groups have streamlined shells, forelimbs modified as flippers that propel their bodies through the water, figure-eight swimming strokes, and large, fully webbed hind feet as rudders. Cheloniids are hard-shelled sea turtles with a bony carapace (top shell) and plastron (bottom shell) with epidermal scutes (scales). In contrast, the leatherback shell of dermochelyids has a greatly reduced bony architecture, and the bones are less firmly articulated; scutes appear in hatchlings, but they are quickly shed, so the bony shell is covered with a thick, leathery skin. Size varies greatly among the seven species; however, commonalities exist in diet and habitat. With some exception, most sea turtles are carnivorous and prefer warm, coastal marine environments. The leatherback sea turtle (Dermochelys coriacea) inhabits pelagic (open ocean) environments. Apparently following the blooms of its jellyfish prey, it moves widely throughout the oceans. The shell lengths of few individuals exceed 1.6 metres (5 feet), although some reportedly reach 2.4 metres (8 feet). Adult and juvenile olive ridleys (Lepidochelys olivacea) are also largely pelagic, but they are known to frequent coastal regions such as bays and estuaries. The olive ridley and its relative, the Kemp’s ridley sea turtle (L. kempii), are small with wide rounded shells. As adults, both species have shells about 58–78 cm (23–31 inches) long. Leatherbacks and ridleys are largely carnivorous and consume a wide variety of crustaceans and mollusks. Loggerhead (Caretta caretta) and green (Chelonia mydas) sea turtles have adult shell lengths between 0.9 and 1.2 metres (3 and 4 feet) long. The loggerhead is carnivorous and prefers coastal marine environments. It has the proportionately largest head of the sea turtles; this feature may be an adaptation that increases its jaw strength in order to crush the shells of large mollusks such as whelks. The green turtle is found in warm coastal waters around the world; however, unlike other sea turtles, it is predominantly herbivorous and feeds on algae or marine grasses. The hawksbill sea turtle (Eretmochelys imbricata) is largely tropical and common in coral reef habitats, where it feeds on sponges and a variety of other invertebrates. The flatback sea turtle (Natator depressa) occurs in the seas between Australia and New Guinea; it also feeds on a variety of invertebrates. The shells of adults of both species range from 90 to 100 cm (35 to 39 inches).\""
      ],
      "metadata": {
        "id": "ga37tHqo_jqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS2xbJv-r_PH",
        "outputId": "bd8bcb89-01dc-40d2-bf4a-77358a951dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "488"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, remove_punctuation=True, lowercase=True, numbers_to_words_or_remove=None, remove_stopwords=False, stemmer=None, lemmatizer=None):\n",
        "  if remove_punctuation:\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  if lowercase:\n",
        "    text = text.lower()\n",
        "\n",
        "  if numbers_to_words_or_remove == \"remove\":\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  if numbers_to_words_or_remove == \"words\":\n",
        "    digits = re.findall(r'\\d+', text)\n",
        "    for digit in digits:\n",
        "        word = num2words(int(digit))\n",
        "        text = text.replace(digit, word)\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if not token in stop_words]\n",
        "\n",
        "  if stemmer:\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  if lemmatizer:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  return set(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "KG39lmvyjiJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = preprocess(text, remove_punctuation=True, lowercase=True,  numbers_to_words_or_remove='remove', remove_stopwords=False, stemmer=None, lemmatizer=None)\n",
        "t2 = preprocess(text, remove_punctuation=True, lowercase=True,  numbers_to_words_or_remove='words', remove_stopwords=False, stemmer=None, lemmatizer=None)\n",
        "t3 = preprocess(text, remove_punctuation=True, lowercase=True,  numbers_to_words_or_remove='remove', remove_stopwords=True, stemmer=None, lemmatizer=None)\n",
        "t4 = preprocess(text, remove_punctuation=True, lowercase=True,  numbers_to_words_or_remove='remove', remove_stopwords=True, stemmer=True, lemmatizer=None)\n",
        "t5 = preprocess(text, remove_punctuation=True, lowercase=True,  numbers_to_words_or_remove='remove', remove_stopwords=True, stemmer=None, lemmatizer=True)"
      ],
      "metadata": {
        "id": "FR_8ijwhoZiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Basic preprocessing without removing stopwords:\", len(t1))\n",
        "print(\"Basic preprocessing without removing stopwords but replacing numbers with words:\", len(t2))\n",
        "print(\"Preprocessing and removing stopwords:\", len(t3))\n",
        "print(\"Removing stopwords and stemmer:\", len(t4))\n",
        "print(\"Removing stopwords and lemmatizer:\", len(t5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Cq_L_WkI3n",
        "outputId": "2e4cc8cd-a015-48ad-acca-47f234d8d140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic preprocessing without removing stopwords 240\n",
            "Basic preprocessing without removing stopwords but replacing numbers with words 254\n",
            "Preprocessing and removing stopwords 200\n",
            "Removing stopwords and stemmer 184\n",
            "Removing stopwords and lemmatizer 189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3LRU6BTpwTP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}